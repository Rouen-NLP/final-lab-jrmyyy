{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "  <br>\n",
    "  <a><img src=\"http://mastersid.univ-rouen.fr/images/logoSID.jpg\" width=\"200\"></a>\n",
    "  <br>\n",
    "Classification des documents du procès des groupes américains du tabac\n",
    "  <br>\n",
    "</h1>\n",
    "<h4 align=\"center\"> Jeremy CATELAIN</h4>\n",
    "\n",
    "## 0. Introduction\n",
    "### Contexte\n",
    "Le gouvernement américain a attaqué en justice cinq grands groupes américains du tabac pour avoir amassé d'importants bénéfices en mentant sur les dangers de la cigarette. Le cigarettiers  se sont entendus dès 1953, pour \"mener ensemble une vaste campagne de relations publiques afin de contrer les preuves de plus en plus manifestes d'un lien entre la consommation de tabac et des maladies graves\". \n",
    "\n",
    "Dans ce procès 14 millions de documents ont été collectés et numérisés. Afin de faciliter l'exploitation de ces documents par les avocats, vous êtes en charge de mettre en place une classification automatique des types de documents. \n",
    "\n",
    "Un échantillon aléatoire des documents a été collecté et des opérateurs ont classé les documents dans des répertoires correspondant aux classes de documents : lettres, rapports, notes, email, etc. Vous avez à votre disposition : \n",
    "\n",
    "- les images de documents : http://data.teklia.com/Images/Tobacco3482.tar.gz\n",
    "- le texte contenu dans les documents obtenu par OCR (reconnaissance automatique) : Tobacco3482-OCR.tar.gz  (dans ce git)\n",
    "- les classes des documents définies par des opérateurs : Tobacco3482.csv (dans ce git)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Le but de ce projet est d'étudier les données que l'on a à notre disposition et trouver un ou des modèles permettant de classifier correctement les documents. Nous commencerons tout d'abord par analyser les données puis étudier quelques modèles de classification. Pour finir, nous allons analyser les résultats. Nous verrons les modèles suivants: \n",
    "- Classifieur bayésien multinomial\n",
    "- MLP \n",
    "- Régression logistique\n",
    "- CNN & Embedding\n",
    "\n",
    "Les données initiales sont des images de divers documents. Ces dernières ont été traitées par un OCR et chaque image a été convertie en texte afin que nous puissons faire de l'analyse de textes et non d'images. Ainsi, les textes que l'on a ne sont pas toujours correctes en terme de parfaite retranscription, ce qui aura une influence sur la qualité de nos classification. \n",
    "\n",
    "### Informations\n",
    "\n",
    "- Ce notebook est un complémentaire du script python (script.py) et du rapport (rapport.md). Contrairement au code du script, le code de ce notebook n'a pas été corrigé pour suivre le style d'écriture PEP8. Il est important de noter que ce code donne les mêmes résultats que le script python même si le format n'est pas le même. Le format a été adapté pour le notebook. Les commentaires se trouvant dans ce notebook sont complémentaires aux commentaires du rapport. Ils visent davantage à commenter les résultats du code plutôt que l'analyse synthétique des résultats (cf rapport).\n",
    "\n",
    "- Le noyau à utiliser est \"env\" (fournit avec ce notebook). Le fichier requirement.txt est également joint. \n",
    "- Ce notebook doit être lu et/ou exécuter dans l'ordre. Seul le script a été adapté pour exécuter un ou plusieurs modèle à la fois.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame as df\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn.model_selection  as model_selection\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras.layers import Dense, Embedding, Input, Dropout, MaxPooling1D, Conv1D, Flatten\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import linear_model\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analyse des données \n",
    "\n",
    "Tout d'abord, nous étudions le fichier csv qui a été fourni avec les données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3482, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Index(['img_path', 'label'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3482</td>\n",
       "      <td>3482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3482</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Memo/2054927900_2054927901.jpg</td>\n",
       "      <td>Memo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              img_path label\n",
       "count                             3482  3482\n",
       "unique                            3482    10\n",
       "top     Memo/2054927900_2054927901.jpg  Memo\n",
       "freq                                 1   620"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classes =  pd.read_csv('data/Tobacco3482.csv', sep = \",\") #Lecture du fichier de données. \n",
    "df_classes.shape #shape du fichier\n",
    "df_classes.columns #le tableau contient deux colonnes: \"img_path\" et \"label\"\n",
    "df_classes.describe() #summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a 3482 données dans le fichier Tobacco3482.csv. Ce fichier contient deux colonnes. On peut compter 10 labels différents. \n",
    "\n",
    "Affichons 5 éléments pris aléatoirement dans le fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>Memo/2076900993.jpg</td>\n",
       "      <td>Memo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>News/2046117930.jpg</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>Resume/50591446-1447.jpg</td>\n",
       "      <td>Resume</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>Memo/2025851048_2025851049.jpg</td>\n",
       "      <td>Memo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>Form/2024027426_7427.jpg</td>\n",
       "      <td>Form</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            img_path   label\n",
       "2260             Memo/2076900993.jpg    Memo\n",
       "2543             News/2046117930.jpg    News\n",
       "3193        Resume/50591446-1447.jpg  Resume\n",
       "2026  Memo/2025851048_2025851049.jpg    Memo\n",
       "853         Form/2024027426_7427.jpg    Form"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classes.sample(n=5) #5 éléments du fichier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe deux colonnes: le chemin d'une image ainsi que le label de cette image (nom du dossier dans lequel l'image se trouve). Ce fichier permet simplement d'obtenir le chemin et le label pour chaque document.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3482, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classes.dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a aucune données manquantes dans le fichier csv. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous disposons des textes contenus dans les documents obtenus par OCR (reconnaissance automatique). Analysons ces fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de documents ayant que 3 mots =  5\n",
      "Nombre de documents ayant moins de 3 mots =  51\n",
      "Pourcentage de documents retirés =  1.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3431.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>160.910230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>124.326764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1213.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  3431.000000\n",
       "mean    160.910230\n",
       "std     124.326764\n",
       "min       3.000000\n",
       "25%      80.000000\n",
       "50%     138.000000\n",
       "75%     204.000000\n",
       "max    1213.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "y= []\n",
    "nb_zeros = 0\n",
    "nb_words = []\n",
    "keys = df_classes[\"label\"] #tous les labels\n",
    "dict_nbwords = dict(zip(keys, [0]*len(keys)))\n",
    "for i in os.listdir('./data/Tobacco3482-OCR'): #sous-dossiers\n",
    "        files=glob.glob('./data/Tobacco3482-OCR/'+i+'/*.txt') #fichiers dans le sous-dossier\n",
    "        for file in files:\n",
    "            f=open(file, 'r')\n",
    "            text = f.read() #le texte\n",
    "            c = len(Counter(text.split())) #nombre de mots\n",
    "            if c>=3: #si il y a au moins trois mots dans le documents\n",
    "                x.append(text)  #le texte\n",
    "                nb_words.append(c) #nombre de mots par documents\n",
    "                dict_nbwords[i] = (c + dict_nbwords[i]) / 2\n",
    "                y.append(i) #nom du sous-dossier (=label)\n",
    "            else: \n",
    "                nb_zeros+=1\n",
    "                \n",
    "nb_words_array = np.array(nb_words) \n",
    "df_nb_words = df(nb_words_array)\n",
    "index = np.where(nb_words_array == nb_words_array.min())\n",
    "print(\"Nombre de documents ayant que 3 mots = \", len(index[0]))\n",
    "print(\"Nombre de documents ayant moins de 3 mots = \", nb_zeros)\n",
    "print(\"Pourcentage de documents retirés = \", np.round(100*(nb_zeros)/3482,1))\n",
    "df_nb_words.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous disposons de 3482 documents. On constate qu'en moyenne, un document a 160 mots différents. Cependant la variance est grande (124). Il est intéressant de regarder alors la médiane qui est de 138. Nous avons retiré seulement 1.5% des documents correspondant aux documents ayant au plus deux mots. On a donc décidé de garder tous les documents ayant au moins trois mots. Ce choix peut être justifié par le fait que deux mots ne suffisent pas pour classer un document. De plus, après avoir regardé ces documents, on constate que les deux seuls mots qu'ils ont, sont ininterprétables (il y a même des documents n'ayant aucun mot). \n",
    "\n",
    "Nous disposons maintenant de 3431 fichiers textes. Visualisons la répartition des différents labels sur l'ensemble de nos documents ainsi que la répartition approximative du nombre de mots par document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "c = Counter(y)\n",
    "ax1.barh(list(c.keys()), list(c.values()))\n",
    "ax1.set_title(\"Répartition des labels\");\n",
    "\n",
    "c = Counter(nb_words)\n",
    "ax2.bar(list(c.keys()), list(c.values()))\n",
    "ax2.set_title(\"Répartition des nombres de mots par documents\");\n",
    "ax2.set_xlabel('Nombres de mots');\n",
    "ax2.set_ylabel('Nombres de documents');\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.barh(list(dict_nbwords.keys()), list(dict_nbwords.values()));\n",
    "plt.title(\"Répartition du nombre de mots moyen par documents par label\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les trois labels les plus représentés sont Email, Letter et Memo. Les labels Resume, News et Note sont ceux qui sont les moins présents. Au regard des valeurs des occurrences, on peut dire que les labels sont plutôt bien partagés dans les données (non présence de sur-représentation). En ce qui concerne le nombre de mots par documents, on remarque que la plupart des documents ont entre 100 et 200 mots. Seuls quelques documents ont un nombre de mots supérieures à 200. On remarque que le nombre moyen de mots par documents ne différent pas considérablement entre les documents de labels différents. Seuls les documents News ont un nombre moyen deux fois plus élevés que les autres. On a donc plus d'informations pour les documents de ce label. Et en général, on a relativement le même quantité d'informations pour chaque label (car l'ordre de grandeur n'est pas très différent selon les labels (entre 50 et 100)). \n",
    "\n",
    "Nous avons mis dans un tableau tous les textes OCR. Ayant peu de données (<1 M d'exemples), nous sommes dans un cas classique de machine learning. On découpe alors nos données de la manière suivante: \n",
    "- 60% pour l'ensemble d'apprentissage.\n",
    "- 20% pour l'ensemble de tests. \n",
    "- 20% pour l'ensemble de validation. \n",
    "\n",
    "Nous allons découper nos données comme décrit ci-dessus et vérifier qu'ils suivent bien la même distribution par rapport aux classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.4, random_state=1)\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(17,5))\n",
    "\n",
    "c = Counter(y_train)\n",
    "ax1.barh(list(c.keys()), list(c.values()))\n",
    "ax1.set_title(\"Répartition des labels train-test\");\n",
    "c = Counter(y_test)\n",
    "ax1.barh(list(c.keys()), list(c.values()))\n",
    "ax1.legend([\"Train\", \"Test\"]);\n",
    "\n",
    "x_test,x_val,y_test,y_val = train_test_split(x_test,y_test, test_size=0.5, random_state=1)\n",
    "c = Counter(y_test)\n",
    "ax2.barh(list(c.keys()), list(c.values()))\n",
    "ax2.set_title(\"Répartition des labels test-val\");\n",
    "c = Counter(y_val)\n",
    "ax2.barh(list(c.keys()), list(c.values()))\n",
    "ax2.legend([\"Test\", \"Val\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données d'apprentissage, de validation et de tests suivent relativement bien la même répartion par rapport aux labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classifieur bayésien multinomial\n",
    "\n",
    "Après avoir analyser nos données, nous allons, à présent, les classifier avec différentes méthodes.\n",
    "\n",
    "Premièrement, nous allons utiliser la représentation TF-IDF pour former un Classifieur bayésien multinomial. Ce classifieur a pour but de maximiser la classification à partir des probabilités jointes (cad occurrence simultanée d'un mot dans un document et de la classe de ce document).\n",
    "\n",
    "Nous utilisons d'abord CountVectorizer qui encode le texte en utilisant les fréquences des mots. Cependant, les mots très fréquents et figurant dans tous les documents auront un poids important alors qu'ils ne sont pas discriminants. On utilise alors la méthode TF-IDF qui prend en compte le nombre de documents dans lesquels un mot donné apparaît. Un mot qui apparaît dans de nombreux documents aura maintenant un poids moins important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire:  57855\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer() #on utilise l'ensemble des mots (pas de max_features pour l'instant)\n",
    "vectorizer.fit(x_train) #apprendre le vocabulaire à partir des documents.\n",
    "print(\"Taille du vocabulaire: \",len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (2058, 57855)\n",
      "Validation:  (687, 57855)\n",
      "Test:  (686, 57855)\n"
     ]
    }
   ],
   "source": [
    "x_train_counts = vectorizer.transform(x_train) #encode chaque document en vecteur.\n",
    "x_val_counts = vectorizer.transform(x_val) #encode chaque document en vecteur.\n",
    "x_test_counts = vectorizer.transform(x_test) #encode chaque document en vecteur.\n",
    "print(\"Train: \", x_train_counts.shape)\n",
    "print(\"Validation: \",x_val_counts.shape)\n",
    "print(\"Test: \",x_test_counts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons construit des matrices de termes-documents avec un vocabulaire de taille 57 855."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le TfidfTransformer est appliqué après le CountVectorizer\n",
    "tf_transformer = TfidfTransformer()\n",
    "#transforme la matrice en une représentation tf-idf\n",
    "x_train_tf = tf_transformer.fit_transform(x_train_counts) \n",
    "x_val_tf= tf_transformer.fit_transform(x_val_counts)\n",
    "x_test_tf= tf_transformer.fit_transform(x_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ayant maintenant obtenu les caractéristiques des documents, nous pouvons construire un Classifieur bayésien multinomial pour essayer de prédire la classe d'un document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_tf,y_train) #apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant évaluer notre modèle sur l'ensemble de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du modèle:  0.5036390101892285\n",
      "Moyenne du score du modèle:  0.4646525512516364\n",
      "Variance du score du modèle:  0.041892738900109125\n"
     ]
    }
   ],
   "source": [
    "score = clf.score(x_val_tf,y_val)\n",
    "print(\"Score du modèle: \", score)\n",
    "\n",
    "cv = model_selection.cross_val_score(clf, x_val_tf, y_val, cv=10)\n",
    "moy = np.mean(cv)\n",
    "var = np.std(cv)\n",
    "print(\"Moyenne du score du modèle: \", moy)\n",
    "print(\"Variance du score du modèle: \", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après une validation croisée sur l'ensemble de validation avec k=10, on trouve que le score du modèle est d'environ 50%, ce qui est relativement faible. La classification est bonne dans moins d'un cas sur 2. Cependant, le score est plus grand que si la classification se faisait aléatoirement (>1/10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des erreurs\n",
    "\n",
    "Le taux d'erreur de classification donne une évaluation de la performance pour toutes les classes. Cependant, les classes ne sont pas identiquement distribuées. Pour avoir une meilleure idée de la performance du classifieur, nous allons utiliser les métriques suivantes: \n",
    "- *metrics.classification_report* qui fournit une analyse détaillée par classe: précision (parmi tous les exemples classés dans la classe X, combien appartiennent réellement à la classe X) et le rappel (parmi tous les exemples appartenant à la classe X, combien sont classés dans la classe X) et le F-Score qui correspond à la moyenne harmonique pondérée de la précision et du rappel.\n",
    "- *metrics.confusion_matrix* qui donnent les confusions entre les classes.\n",
    "\n",
    "Nous utilisons l'ensemble test pour étudier les erreurs du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Advertisement       1.00      0.10      0.18        40\n",
      "        Email       0.97      0.89      0.93       131\n",
      "         Form       0.68      0.43      0.53        84\n",
      "       Letter       0.35      0.83      0.49       109\n",
      "         Memo       0.43      0.83      0.56       128\n",
      "         News       1.00      0.03      0.05        40\n",
      "         Note       0.00      0.00      0.00        28\n",
      "       Report       0.00      0.00      0.00        49\n",
      "       Resume       0.00      0.00      0.00        20\n",
      "   Scientific       0.00      0.00      0.00        57\n",
      "\n",
      "    micro avg       0.51      0.51      0.51       686\n",
      "    macro avg       0.44      0.31      0.27       686\n",
      " weighted avg       0.52      0.51      0.44       686\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcatelain/Bureau/TA/env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Rapport de classification\n",
    "y_pred = clf.predict(x_test_tf)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les  métriques calculées ne sont pas très bonnes dans l'ensemble. Seulement trois labels ont une précision de plus de 90% (Email, Advertissement et News (même 100% pour les deux derniers)). 4 labels ont une précision de 0% ce qui témoigne du mauvais apprentissage de notre modèle. Nous avons donc un cas de sur-apprentissage. On peut faire les mêmes conclusions pour les métriques Rappel et F1-score. Notre modèle ne classe donc pas bien de nouvelles données. \n",
    "\n",
    "Le résultat de macro average est inférieur à celui de micro pour les 3 métriques, donc les documents appartenant aux labels les moins présents dans la base de données sont mal classées, alors qu'ils sont correctement classées pour ceux apparaissant le plus.\n",
    "\n",
    "Calculons la matrice de confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion: \n",
      " [[  4   1   2  13  20   0   0   0   0   0]\n",
      " [  0 116   0   8   7   0   0   0   0   0]\n",
      " [  0   1  36  12  35   0   0   0   0   0]\n",
      " [  0   0   0  90  19   0   0   0   0   0]\n",
      " [  0   0   0  22 106   0   0   0   0   0]\n",
      " [  0   0   4  30   5   1   0   0   0   0]\n",
      " [  0   1   5   9  13   0   0   0   0   0]\n",
      " [  0   0   1  32  16   0   0   0   0   0]\n",
      " [  0   0   0  20   0   0   0   0   0   0]\n",
      " [  0   0   5  24  28   0   0   0   0   0]]\n",
      "Matrice de confusion normalisé (en %): \n",
      " [[10  0  2 11 15  0  0  0  0  0]\n",
      " [ 0 88  0  7  5  0  0  0  0  0]\n",
      " [ 0  0 42 11 27  0  0  0  0  0]\n",
      " [ 0  0  0 82 14  0  0  0  0  0]\n",
      " [ 0  0  0 20 82  0  0  0  0  0]\n",
      " [ 0  0  4 27  3  2  0  0  0  0]\n",
      " [ 0  0  5  8 10  0  0  0  0  0]\n",
      " [ 0  0  1 29 12  0  0  0  0  0]\n",
      " [ 0  0  0 18  0  0  0  0  0  0]\n",
      " [ 0  0  5 22 21  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Matrice de confusion\n",
    "matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matrice de confusion: \\n\",matrice_confusion)\n",
    "\n",
    "#normaliser la matrice de confusion\n",
    "matrice_confusion = matrice_confusion / matrice_confusion.astype(np.float).sum(axis=1)\n",
    "matrice_confusion = np.floor(matrice_confusion*100)\n",
    "print(\"Matrice de confusion normalisé (en %): \\n\",matrice_confusion.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate les mêmes résultats avec la matrice de confusion. Cette dernière n'étant pas diagonale, le classifieur n'est donc pas très bon.\n",
    "\n",
    "En conclusion, notre modèle ne fonctionne pas très bien malgré un score de 50%. Nous allons donc chercher à l'améliorer en optimisant les hyperparamètres. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation des hyperparamètres\n",
    "\n",
    "Nous allons optimiser les paramètres de la classification en effectuant une recherche sur grille à l'aide de GridSearchCV. Voici les différents paramètres que l'on va traiter: \n",
    "- alpha, le paramètre de lissage pour le classifieur  bayésien. \n",
    "- max_features, le nombre maximal de termes les plus fréquents à prendre en compte\n",
    "- max_df, le seuil à partir duquel on ignore les termes (ceux qui apparaissent dans plus de max_df% des documents).\n",
    "- utilisation de la répresentation idf ou non. \n",
    "- définition de ngram pour le CountVectorizer of TfIDF transformer (unigramme, bigramme..). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Pipeline: ['vect', 'tfidf', 'clf']\n",
      "Paramètres:\n",
      "{'clf__alpha': (1, 0.5, 0.1, 0.01, 0.001),\n",
      " 'tfidf__use_idf': (True, False),\n",
      " 'vect__max_df': (0.3, 0.4, 0.5, 0.7, 0.8),\n",
      " 'vect__max_features': (1000, 1500, 2000, 2500),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 3 folds for each of 400 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   55.5s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:  7.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'vect__max_df': (0.3, 0.4, 0.5, 0.7, 0.8), 'vect__max_features': (1000, 1500, 2000, 2500), 'tfidf__use_idf': (True, False), 'vect__ngram_range': ((1, 1), (1, 2)), 'clf__alpha': (1, 0.5, 0.1, 0.01, 0.001)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fait en 437.779s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# affichage des logs\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.3, 0.4, 0.5, 0.7, 0.8),\n",
    "    'vect__max_features' : (1000, 1500, 2000, 2500),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)), # unigramme ou bigramme\n",
    "    'clf__alpha': (1, 0.5, 0.1, 0.01, 0.001)\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=2, cv=3)\n",
    "print(\"Performing grid search...\")\n",
    "print(\"Pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"Paramètres:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(\"fait en %0.3fs\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur score: 0.737\n",
      "Meilleur paramètres:\n",
      "\tclf__alpha: 0.1\n",
      "\ttfidf__use_idf: False\n",
      "\tvect__max_df: 0.4\n",
      "\tvect__max_features: 1000\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Meilleur score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Meilleur paramètres:\")\n",
    "best_estimator = grid_search.best_estimator_\n",
    "best_parameters = best_estimator.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir faire une recherche des meilleurs paramètres, on trouve un meilleur modèle avec un score de 0.72. Ce nouveau modèle n'utilise pas la représentation idf et ignore les termes apparaissant dans plus 55% des documents. Les bigrammes sont utilisés. \n",
    "\n",
    "Evaluons à présent notre nouveau modèle sur l'ensemble de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du modèle:  0.7088791848617176\n",
      "Moyenne du score du modèle:  0.7028665339528963\n",
      "Variance du score du modèle:  0.04886320575790256\n"
     ]
    }
   ],
   "source": [
    "score = best_estimator.score(x_val,y_val)\n",
    "print(\"Score du modèle: \", score)\n",
    "\n",
    "cv = model_selection.cross_val_score(best_estimator, x_val, y_val, cv=10)\n",
    "moy = np.mean(cv)\n",
    "var = np.std(cv)\n",
    "print(\"Moyenne du score du modèle: \", moy)\n",
    "print(\"Variance du score du modèle: \", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score est très proche du score trouvé avec l'ensemble d'apprentissage (73%). En moyenne avec la validation-croisée, un score de 68% est trouvé, ce qui est plutôt correct. \n",
    "\n",
    "Etudions les erreurs du modèle avec l'ensemble test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Advertisement       0.73      0.68      0.70        40\n",
      "        Email       0.94      0.95      0.95       131\n",
      "         Form       0.74      0.79      0.76        84\n",
      "       Letter       0.63      0.77      0.69       109\n",
      "         Memo       0.70      0.71      0.71       128\n",
      "         News       0.68      0.75      0.71        40\n",
      "         Note       0.53      0.32      0.40        28\n",
      "       Report       0.55      0.37      0.44        49\n",
      "       Resume       0.95      1.00      0.98        20\n",
      "   Scientific       0.73      0.61      0.67        57\n",
      "\n",
      "    micro avg       0.74      0.74      0.74       686\n",
      "    macro avg       0.72      0.69      0.70       686\n",
      " weighted avg       0.73      0.74      0.73       686\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rapport de classification\n",
    "y_pred =  best_estimator.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont beaucoup mieux. On retrouve le même score qu'avec les données d'apprentissage. En moyenne, le rappel, la précésion et le f1-score sont de 73%. Lesrésultatsde macro et de micro average pour les 3 métriques sont plutôt similaires donc le système prédit bien tous les labels, peu importe leur répartition dans la base de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion: \n",
      " [[ 27   2   3   4   2   1   1   0   0   0]\n",
      " [  0 125   0   4   2   0   0   0   0   0]\n",
      " [  2   1  66   5   6   0   3   1   0   0]\n",
      " [  0   0   2  84  14   1   0   6   0   2]\n",
      " [  0   4   2  19  91   1   2   5   0   4]\n",
      " [  3   0   2   0   1  30   1   2   0   1]\n",
      " [  3   1   2   7   5   0   9   0   1   0]\n",
      " [  1   0   0   8   6  10   0  18   0   6]\n",
      " [  0   0   0   0   0   0   0   0  20   0]\n",
      " [  1   0  12   3   3   1   1   1   0  35]]\n",
      "Matrice de confusion normalisé (en %): \n",
      " [[ 67   1   3   3   1   2   3   0   0   0]\n",
      " [  0  95   0   3   1   0   0   0   0   0]\n",
      " [  5   0  78   4   4   0  10   2   0   0]\n",
      " [  0   0   2  77  10   2   0  12   0   3]\n",
      " [  0   3   2  17  71   2   7  10   0   7]\n",
      " [  7   0   2   0   0  75   3   4   0   1]\n",
      " [  7   0   2   6   3   0  32   0   5   0]\n",
      " [  2   0   0   7   4  25   0  36   0  10]\n",
      " [  0   0   0   0   0   0   0   0 100   0]\n",
      " [  2   0  14   2   2   2   3   2   0  61]]\n"
     ]
    }
   ],
   "source": [
    "# Matrice de confusion\n",
    "matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matrice de confusion: \\n\",matrice_confusion)\n",
    "\n",
    "#normaliser la matrice de confusion\n",
    "matrice_confusion = matrice_confusion / matrice_confusion.astype(np.float).sum(axis=1)\n",
    "matrice_confusion = np.floor(matrice_confusion*100)\n",
    "print(\"Matrice de confusion normalisé (en %): \\n\",matrice_confusion.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate les mêmes résultats avec la matrice de confusion. Cette dernière est plutôt diagonale, le classifieur est donc relativement bon.\n",
    "\n",
    "En conclusion, nous avons trouvé un meilleur modèle grâce à l'optimisation des hyperparamètres. En effet, le score a été augmenté de plus de 20 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Classification avec un MLP\n",
    "\n",
    "Nous allons utilisé un autre type de modèle: celui des réseaux de neurones. Ces derniers peuvent être formés pour apprendre à la fois la représentation vectorielle des mots (au lieu de tf-idf) et la façon de classer les documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.4, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.4, random_state=1)\n",
    "x_test,x_val,y_test,y_val = train_test_split(x_test,y_test, test_size=0.5, random_state=1)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=best_parameters[\"vect__max_features\"], max_df=best_parameters[\"vect__max_df\"], ngram_range=best_parameters[\"vect__ngram_range\"]) #on réutilise les meilleurs paramètres\n",
    "vectorizer.fit(x_train) \n",
    "\n",
    "x_train_counts = vectorizer.transform(x_train) \n",
    "x_val_counts = vectorizer.transform(x_val) \n",
    "x_test_counts = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons tout d'abord utiliser un Perceptron multicouches avec le sklearn.neural_network.MLPClassifier. Après avoir testé différentes valeurs pour les paramètres suivants: \n",
    "- le nombre de couches et le nombre de neurones par couches\n",
    "- le nombre d'itérations\n",
    "- alpha, paramètre de régularisation\n",
    "- fonction d'activation\n",
    "\n",
    "le meilleur modèle trouvé est le suivant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.05549440\n",
      "Iteration 2, loss = 0.33497034\n",
      "Iteration 3, loss = 0.17652233\n",
      "Iteration 4, loss = 0.10359152\n",
      "Iteration 5, loss = 0.07480610\n",
      "Iteration 6, loss = 0.05526966\n",
      "Iteration 7, loss = 0.04362564\n",
      "Iteration 8, loss = 0.03677205\n",
      "Iteration 9, loss = 0.03214060\n",
      "Iteration 10, loss = 0.02747560\n",
      "Iteration 11, loss = 0.02523430\n",
      "Iteration 12, loss = 0.02376967\n",
      "Iteration 13, loss = 0.02130671\n",
      "Iteration 14, loss = 0.02045178\n",
      "Iteration 15, loss = 0.02017911\n",
      "Iteration 16, loss = 0.01856318\n",
      "Iteration 17, loss = 0.01754102\n",
      "Iteration 18, loss = 0.01737920\n",
      "Iteration 19, loss = 0.01685487\n",
      "Iteration 20, loss = 0.01598886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcatelain/Bureau/TA/env/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size=10, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(300,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=20, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(activation=\"relu\",hidden_layer_sizes=(300,), max_iter=20, solver='adam', batch_size=10, verbose=1)\n",
    "\n",
    "mlp.fit(x_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons donc pris un MLP avec une couche de 300 neurones et utilisé la fonction d'activation \"relu\" ainsi que l'optimiseur \"adam\" (rmsprop + momentum) avec un batch de taille 10 et un paramètre de régularisation valant 0.0001. \n",
    "\n",
    "Au bout de 20 itérations, le coût est de seulement 0.01. Evaluons à présent notre modèle sur l'ensemble de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du modèle:  0.7787481804949054\n"
     ]
    }
   ],
   "source": [
    "score = mlp.score(x_val_counts,y_val)\n",
    "print(\"Score du modèle: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score est très bon (78%). Il est bien meilleur que les précédents modèles. Etudions les erreurs du modèle avec l'ensemble test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Advertisement       0.71      0.75      0.73        40\n",
      "        Email       0.93      0.97      0.95       131\n",
      "         Form       0.82      0.86      0.84        84\n",
      "       Letter       0.73      0.81      0.77       109\n",
      "         Memo       0.84      0.68      0.75       128\n",
      "         News       0.81      0.75      0.78        40\n",
      "         Note       0.51      0.68      0.58        28\n",
      "       Report       0.53      0.51      0.52        49\n",
      "       Resume       1.00      1.00      1.00        20\n",
      "   Scientific       0.61      0.58      0.59        57\n",
      "\n",
      "    micro avg       0.77      0.77      0.77       686\n",
      "    macro avg       0.75      0.76      0.75       686\n",
      " weighted avg       0.78      0.77      0.77       686\n",
      "\n",
      "Matrice de confusion normalisé (en %): \n",
      " [[ 75   0   2   0   0   0  21   0   0   1]\n",
      " [  0  96   0   2   0   0   3   0   0   0]\n",
      " [  5   0  85   3   1   0  14   0   0   0]\n",
      " [  2   2   1  80   7   2   0   8   0   1]\n",
      " [  5   3   4  15  67   0  10  12   0   8]\n",
      " [  5   0   0   0   0  75   0   6   0   7]\n",
      " [  5   0   0   1   2   0  67   0   0   1]\n",
      " [  5   0   2   4   0  15   0  51   0  15]\n",
      " [  0   0   0   0   0   0   0   0 100   0]\n",
      " [  2   0   8   0   1   0  14  18   0  57]]\n"
     ]
    }
   ],
   "source": [
    "# Rapport de classification\n",
    "y_pred =  mlp.predict(x_test_counts)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matrice de confusion\n",
    "matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "#normaliser la matrice de confusion\n",
    "matrice_confusion = matrice_confusion / matrice_confusion.astype(np.float).sum(axis=1)\n",
    "matrice_confusion = np.floor(matrice_confusion*100)\n",
    "print(\"Matrice de confusion normalisé (en %): \\n\",matrice_confusion.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs des précisions sont très bonnes. Le modèle prédit bien tous les labels (la précision est légèrement plus faible pour le label Note). En moyenne, le rappel, la précision et le f1-score sont de 76%, ce qui est meilleur que les précédents modèles. Les résultats de macro et de micro average pour les 3 métriques sont plutôt similaires donc le système prédit bien tous les labels, peu importe leur répartition dans la base de données. \n",
    "\n",
    "On constate les mêmes résultats avec la matrice de confusion. Cette dernière est plutôt diagonale, le classifieur est donc relativement bon (et meilleur que les précedents modèles). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification avec régression logistique\n",
    "Nous allons maintenant étudier la méthode de régression logistique (régression + fonction logistique) qui diffère légèrement du Classifieur multinomial bayésien. En effet, celui-ci a pour but de maximiser la classification à partir des probabilités conditionnelles (et non jointes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.4, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.4, random_state=1)\n",
    "x_test,x_val,y_test,y_val = train_test_split(x_test,y_test, test_size=0.5, random_state=1)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=best_parameters[\"vect__max_features\"], max_df=best_parameters[\"vect__max_df\"], ngram_range=best_parameters[\"vect__ngram_range\"]) #on réutilise les meilleurs paramètres\n",
    "vectorizer.fit(x_train) \n",
    "\n",
    "x_train_counts = vectorizer.transform(x_train) \n",
    "x_val_counts = vectorizer.transform(x_val) \n",
    "x_test_counts = vectorizer.transform(x_test)\n",
    "\n",
    "\n",
    "lr = linear_model.LogisticRegression()\n",
    "lr.fit(x_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons le score du modèle sur les données d'évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du modèle:  0.7583697234352256\n"
     ]
    }
   ],
   "source": [
    "score = lr.score(x_val_counts,y_val)\n",
    "print(\"Score du modèle: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score est de 76%, ce qui est très bon en comparant avec les résultats précédents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Advertisement       0.68      0.70      0.69        40\n",
      "        Email       0.94      0.97      0.95       131\n",
      "         Form       0.83      0.85      0.84        84\n",
      "       Letter       0.74      0.79      0.76       109\n",
      "         Memo       0.80      0.77      0.79       128\n",
      "         News       0.76      0.70      0.73        40\n",
      "         Note       0.53      0.71      0.61        28\n",
      "       Report       0.50      0.39      0.44        49\n",
      "       Resume       1.00      1.00      1.00        20\n",
      "   Scientific       0.61      0.54      0.57        57\n",
      "\n",
      "    micro avg       0.77      0.77      0.77       686\n",
      "    macro avg       0.74      0.74      0.74       686\n",
      " weighted avg       0.77      0.77      0.77       686\n",
      "\n",
      "Matrice de confusion normalisé (en %): \n",
      " [[ 70   1   2   0   0   0  21   2   0   1]\n",
      " [  0  96   0   2   0   0   3   0   0   0]\n",
      " [  7   0  84   3   0   0  14   2   0   1]\n",
      " [  2   2   0  78  10   2   0   6   0   1]\n",
      " [  2   2   2  11  77   2   7   8   0   7]\n",
      " [  2   0   0   0   0  70   7   6   0   7]\n",
      " [  2   0   1   2   1   0  71   0   0   1]\n",
      " [ 10   0   2   6   2  15   0  38   0  14]\n",
      " [  0   0   0   0   0   0   0   0 100   0]\n",
      " [  5   0   9   0   3   2  10  14   0  54]]\n"
     ]
    }
   ],
   "source": [
    "# Rapport de classification\n",
    "y_pred =  lr.predict(x_test_counts)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matrice de confusion\n",
    "matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "#normaliser la matrice de confusion\n",
    "matrice_confusion = matrice_confusion / matrice_confusion.astype(np.float).sum(axis=1)\n",
    "matrice_confusion = np.floor(matrice_confusion*100)\n",
    "print(\"Matrice de confusion normalisé (en %): \\n\",matrice_confusion.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs des précisions sont très bonnes. Le modèle prédit bien tous les labels (la précision est plus de 50% pour tous les labels). En moyenne, le rappel, la précision et le f1-score sont de 77%. Les résultats de macro et de micro average pour les 3 métriques sont plutôt similaires donc le système prédit bien tous les labels, peu importe leur répartition dans la base de données. On constate les mêmes résultats avec la matrice de confusion. Cette dernière est plutôt diagonale, le classifieur est donc relativement bon (et meilleur que les précedents modèles). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification avec un CNN\n",
    "\n",
    "Nous allons maintenant créer un classifieur de texte avec un réseau de neurones à convolutions en utilisant un plongement de mots (word embedddings). On utilise un réseau à convolutions avec une grille d'une dimension car nos données sont des séquences. Ce type de réseau part du principe que la détection de feature est important pour la classficiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_train_test(train_raw_text, test_raw_text):\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
    "\n",
    "    tokenizer.fit_on_texts(list(train_raw_text))\n",
    "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
    "    return sequence.pad_sequences(train_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           sequence.pad_sequences(test_tokenized, maxlen=MAX_TEXT_LENGTH)\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    inputs = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "    model = Embedding(MAX_FEATURES, EMBED_SIZE, input_length=MAX_TEXT_LENGTH)(inputs)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Conv1D(NUM_FILTERS, 5, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=30)(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(n_out, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inputs, outputs=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choississons les paramètres du modèles: \n",
    "- la taille du vocabulaire\n",
    "- la longueur maximale des textes\n",
    "- la taille de la couche Embedding\n",
    "- le nombre de filtres pour le CNN\n",
    "- la taille du batch\n",
    "\n",
    "Après plusieurs tests, les paramètres amenant à un meilleur score sont les suivants: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.4, random_state=1)\n",
    "\n",
    "# Model parameters\n",
    "MAX_FEATURES = 2000 #taille du vocabulaire\n",
    "MAX_TEXT_LENGTH = 1213 #nombre max approximative de mots trouvé durant l'analyse\n",
    "EMBED_SIZE  = 300 \n",
    "BATCH_SIZE = 16\n",
    "NUM_FILTERS = 125\n",
    "EPOCHS = 10\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On transforme les textes en séquence et on effectue un padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2058 1373\n"
     ]
    }
   ],
   "source": [
    "# Get the list of different classes\n",
    "CLASSES_LIST = np.unique(y_train)\n",
    "n_out = len(CLASSES_LIST)\n",
    "\n",
    "# Convert clas string to index\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(CLASSES_LIST)\n",
    "y_train = le.transform(y_train) \n",
    "y_test = le.transform(y_test) \n",
    "train_y_cat = np_utils.to_categorical(y_train, n_out)\n",
    "\n",
    "# get the textual data in the correct format for NN\n",
    "x_vec_train, x_vec_test = get_train_test(x_train, x_test)\n",
    "print(len(x_vec_train), len(x_vec_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons préparer les données. On peut maintenant entraîner notre modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1213)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1213, 300)         600000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1213, 300)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1213, 125)         187625    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 125)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                50010     \n",
      "=================================================================\n",
      "Total params: 837,635\n",
      "Trainable params: 837,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1852 samples, validate on 206 samples\n",
      "Epoch 1/10\n",
      "1852/1852 [==============================] - 36s 20ms/step - loss: 1.9268 - acc: 0.2878 - val_loss: 1.7416 - val_acc: 0.3641\n",
      "Epoch 2/10\n",
      "1852/1852 [==============================] - 35s 19ms/step - loss: 1.5434 - acc: 0.4433 - val_loss: 1.2447 - val_acc: 0.5680\n",
      "Epoch 3/10\n",
      "1852/1852 [==============================] - 35s 19ms/step - loss: 0.9127 - acc: 0.6868 - val_loss: 0.8718 - val_acc: 0.7330\n",
      "Epoch 4/10\n",
      "1852/1852 [==============================] - 35s 19ms/step - loss: 0.5547 - acc: 0.8207 - val_loss: 0.7878 - val_acc: 0.6990\n",
      "Epoch 5/10\n",
      "1852/1852 [==============================] - 35s 19ms/step - loss: 0.3659 - acc: 0.8877 - val_loss: 0.7939 - val_acc: 0.7476\n",
      "Epoch 6/10\n",
      "1852/1852 [==============================] - 35s 19ms/step - loss: 0.2204 - acc: 0.9476 - val_loss: 0.7769 - val_acc: 0.7476\n",
      "Epoch 7/10\n",
      "1852/1852 [==============================] - 35s 19ms/step - loss: 0.1553 - acc: 0.9633 - val_loss: 0.7935 - val_acc: 0.7476\n",
      "Epoch 8/10\n",
      "1852/1852 [==============================] - 38s 20ms/step - loss: 0.1106 - acc: 0.9741 - val_loss: 0.8271 - val_acc: 0.7524\n",
      "Epoch 9/10\n",
      "1852/1852 [==============================] - 36s 20ms/step - loss: 0.0916 - acc: 0.9789 - val_loss: 0.8422 - val_acc: 0.7621\n",
      "Epoch 10/10\n",
      "1852/1852 [==============================] - 36s 20ms/step - loss: 0.0722 - acc: 0.9806 - val_loss: 0.8894 - val_acc: 0.7573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f91139ce320>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the NN topology\n",
    "model = get_model()\n",
    "\n",
    "callbacks = [ModelCheckpoint(\"weights.hdf5\", save_best_only=True, save_weights_only=True, monitor='val_acc', mode='max')]\n",
    "\n",
    "\n",
    "model.fit(x_vec_train, train_y_cat,batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, validation_split=VALIDATION_SPLIT, callbacks=callbacks)\n",
    "\n",
    "model.save('modele.h5')  \n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après 10 itérations, le coût a légèrement diminué et la précision est de 98%, signifiant que le modèle a très bien appris les données d'apprentissage.\n",
    "\n",
    "Evaluons notre modèle sur les données tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du modèle: 0.7428987618353969\n"
     ]
    }
   ],
   "source": [
    "model = load_model('modele.h5')\n",
    "model.load_weights('weights.hdf5')\n",
    "\n",
    "y_predicted = model.predict(x_vec_test).argmax(1)\n",
    "print(\"Score du modèle:\", accuracy_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score est de 74%, ce qui est cohérent avec les résultats des autres méthodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.52      0.55        93\n",
      "           1       0.91      0.94      0.92       250\n",
      "           2       0.73      0.73      0.73       165\n",
      "           3       0.77      0.79      0.78       223\n",
      "           4       0.83      0.83      0.83       247\n",
      "           5       0.72      0.66      0.69        77\n",
      "           6       0.49      0.51      0.50        68\n",
      "           7       0.47      0.49      0.48       107\n",
      "           8       0.98      0.96      0.97        47\n",
      "           9       0.57      0.57      0.57        96\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1373\n",
      "   macro avg       0.71      0.70      0.70      1373\n",
      "weighted avg       0.74      0.74      0.74      1373\n",
      "\n",
      "Matrice de confusion normalisé (en %): \n",
      " [[51  1  3  1  0  6 35  0  0  2]\n",
      " [ 0 93  0  3  0  0  5  1  0  0]\n",
      " [10  2 72  1  3  0  7  5  0  7]\n",
      " [ 0  0  2 78  6  1  1 18  0  4]\n",
      " [ 1  3  3  7 82  3  2  3  0  2]\n",
      " [ 8  0  1  1  1 66  0  5  0  4]\n",
      " [ 9  0  5  2  1  0 51  0  0  4]\n",
      " [ 4  0  4  5  2 10  0 48  0 17]\n",
      " [ 0  0  0  0  0  0  0  0 95  2]\n",
      " [ 0  0  5  0  2  3  1 18  2 57]]\n"
     ]
    }
   ],
   "source": [
    "# Rapport de classification\n",
    "print(classification_report(y_test, y_predicted))\n",
    "\n",
    "# Matrice de confusion\n",
    "matrice_confusion = confusion_matrix(y_test, y_predicted)\n",
    "#normaliser la matrice de confusion\n",
    "matrice_confusion = matrice_confusion / matrice_confusion.astype(np.float).sum(axis=1)\n",
    "matrice_confusion = np.floor(matrice_confusion*100)\n",
    "print(\"Matrice de confusion normalisé (en %): \\n\",matrice_confusion.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs des précisions sont très bonnes. Le modèle prédit bien tous les labels (la précision est plus de 50% pour tous les labels). En moyenne, le rappel, la précision et le f1-score sont de 74%. Les résultats de macro et de micro average pour les 3 métriques sont plutôt similaires donc le système prédit bien tous les labels, peu importe leur répartition dans la base de données. On constate les mêmes résultats avec la matrice de confusion. Cette dernière est plutôt diagonale, le classifieur est donc relativement bon (et meilleur que les précedents modèles). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "### Analyses des résultats\n",
    "Voici les scores trouvés pour les différents modèles utilisés. \n",
    "\n",
    "| Modèle      |     Weight average    |   Score |\n",
    "| ------------- |: -------------: | ---------: |\n",
    "| MultinomialNB      |        73%        |      71% |\n",
    "| MLPClassifier        |        78%        |      78% |\n",
    "| LogisticRegression      |       77%        |      76% |\n",
    "| CNN + Embedding      |        74%        |      74% |\n",
    "\n",
    "Nous pouvons donc dire que les modèles crées donnent relativement les mêmes résultats. Le meilleur modèle semble être celui de MLP. Le moins bon est celui avec le classifieur bayéssien (alors qu'il a été optimisé). La représentation TF_IDF ainsi que les probabilités jointes ne semblent donc pas être les meilleures méthodes à appliquer pour faire de la classification sur nos données, alors que les réseaux de neurones (MLP ou CNN) et la régression semble être les meilleurs méthodes.\n",
    "\n",
    "En moyenne, on peut dire que la précision d'un modèle est de 75%. En ayant créée 4 modèles, on peut voir que les précisions sont très proches même avec une optimisation. On peut donc supposer que l'on est proche du niveau de précision maximale qu'un modèle peut atteindre avec ces données. \n",
    "\n",
    "On a aussi pu remarquer que les modèles prédisent très bien certaines classes contrairement à d'autres. Pour toutes les méthodes, les meilleurs labels prédits sont Email et Resume. Les moins bien prédits sont Note et Report. Ces résultats sont intéressants: \n",
    "\n",
    "* Email est le label le plus représenté alors que Resumé est celui le moins représenté. Cependant, ils sont tous les deux les mieux prédit. Le résultat est donc dû à la qualité des textes OCR ou de la quantité d'informations dans les documents. On a pu remarqué que Resume faisait partie des documents ayant le plus de mots par documents tandis qu'il s'agit du contraire pour Email. On a donc deux cas opposés ayant la même qualité de prédiction. On en conclut que: Resume est peu représenté mais sur ce peu de documents, les textes contiennent beaucoup de mots et probablement de bonnes qualités. En ce qui concerne Email, ce label a beaucoup de documents mais chaque document contient très peu de mots. Cependant, il est probable que ces mots soient de très bonnes qualités en terme d'OCR ou d'informations. Cette analyse est plutôt logique car les emails sont des textes courts facile à traiter puisqu'il s'agit d'écritures numériques.\n",
    "\n",
    "* Note et Report sont moyennement représentés. Report a légèrement plus de documents. Cependant, Note a beaucoup de mots que Report. Leurs scores sont de 50%. Ce résultat n'est pas mauvais mais n'est pas le meilleur. Ce score est correct puisqu'il est supérieur à l a probabilité de tirer aléatoirement un label parmi les 10. On peut dire que les documents Note ont peu de mots et que ces derniers sont de mauvaises qualités en terme d'OCR ou d'informations. Il est aussi probable que les documents Report contiennent beaucoup de mots mais que très peu soient de qualités. \n",
    "\n",
    "On peut donc dire que les résultats sont biaisés par la qualité de la technique OCR mais aussi de la qualité des informations dans les documents. \n",
    "\n",
    "\n",
    "\n",
    "### Piste d'améliorations\n",
    "\n",
    "Nous avons pu voir que le modèle Perceptron Multi-Couche est le meilleur. On pourrait l'améliorer davantage en utilisant Keras avec des couches Dense et d'autres fonctions d'activation que celles proposées par sklearn.MLP. Nous ne connaissons pas d'autres modèles pour traiter ce genre de problème (classification de texte). La meilleur méthode pour améliorer nos précisions est la modification des données traitées. Il serait notamment judicieux d'essayer d'améliorer  la technique OCR utilisée. En effet, certaines images ont été très mal retranscriptes. Certaines textes sont vides ou ininterprétables.\n",
    "\n",
    "La technique OCR pourrait être par exemple un réseau de neurones à convolutions en deux dimensions (traiter les caractéristiques) associé à un réseau de neurones récurrents (pour traiter le contexte). On pourrait tester cette technique avec un modèle CTC (traitement de séquences). Par exemple, pour le projet du cours de Sequential Data Analysis, le modèle que j'ai crée (CNN+RNN avec CTC) a atteint un taux de bonne reconnaissance de mots sur des images égal à 60% et un taux de bonne reconnaissance de caractère égal à 73% (résultats sont cohérents avec ceux trouvés par les chercheurs dans ce domaine). \n",
    "\n",
    "De plus, il serait aussi nécéssaire d'avoir des données avec une meilleur répartition. Il faudrait alors avoir plus de données afin d'avoir une répartition des labels plutôt similaires dans les données.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
